#!/usr/bin/env python3
"""
X/Twitter æœ€åå°è¯• - åˆ›æ„æ–¹æ³•
ä½¿ç”¨æœç´¢å¼•æ“ã€ç¤¾äº¤åª’ä½“èšåˆç­‰éå¸¸è§„æ‰‹æ®µ
"""

import urllib.request
import urllib.parse
import re
import ssl
import json
from datetime import datetime

def try_duckduckgo(username: str):
    """ä½¿ç”¨ DuckDuckGo æœç´¢"""
    print("    å°è¯• DuckDuckGo...", end=" ")
    try:
        query = f"twitter.com/{username} followers"
        url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)

        context = ssl.create_default_context()
        with urllib.request.urlopen(req, timeout=15, context=context) as r:
            html = r.read().decode('utf-8', errors='ignore')

        # æŸ¥æ‰¾ç²‰ä¸æ•°
        patterns = [
            rf'{username}.*?([\d,.]+[KMB]?)\s*followers',
            rf'@?{username}.*?([\d,.]+)\s*followers',
        ]

        for pattern in patterns:
            match = re.search(pattern, html, re.IGNORECASE)
            if match:
                count = match.group(1).replace(',', '')
                if 'K' in count:
                    return int(float(count.replace('K', '')) * 1000)
                elif 'M' in count:
                    return int(float(count.replace('M', '')) * 1000000)
                else:
                    return int(float(count))
    except Exception as e:
        print(f"âŒ")
    return None

def try_bing_search(username: str):
    """ä½¿ç”¨ Bing æœç´¢"""
    print("    å°è¯• Bing...", end=" ")
    try:
        query = f"site:twitter.com {username} followers"
        url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        req = urllib.request.Request(url, headers=headers)

        context = ssl.create_default_context()
        with urllib.request.urlopen(req, timeout=15, context=context) as r:
            html = r.read().decode('utf-8', errors='ignore')

        # æŸ¥æ‰¾æ¨¡å¼
        patterns = [
            rf'{username}.*?([\d,.]+[KMB]?) followers',
            rf'([\d,.]+)\s*followers.*?{username}',
        ]

        for pattern in patterns:
            match = re.search(pattern, html, re.IGNORECASE)
            if match:
                count = match.group(1).replace(',', '')
                if 'K' in count:
                    return int(float(count.replace('K', '')) * 1000)
                elif 'M' in count:
                    return int(float(count.replace('M', '')) * 1000000)
                else:
                    try:
                        return int(float(count))
                    except:
                        pass
    except Exception as e:
        print(f"âŒ")
    return None

def try_yandex_search(username: str):
    """ä½¿ç”¨ Yandex æœç´¢ (ä¿„ç½—æ–¯æœç´¢å¼•æ“)"""
    print("    å°è¯• Yandex...", end=" ")
    try:
        url = f"https://yandex.com/search/?text=twitter.com/{username}+followers"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)

        context = ssl.create_default_context()
        with urllib.request.urlopen(req, timeout=15, context=context) as r:
            html = r.read().decode('utf-8', errors='ignore')

        # æŸ¥æ‰¾
        match = re.search(rf'{username}.*?([\d,.]+[KMB]?) followers', html, re.IGNORECASE)
        if match:
            count = match.group(1).replace(',', '')
            if 'K' in count:
                return int(float(count.replace('K', '')) * 1000)
            elif 'M' in count:
                return int(float(count.replace('M', '')) * 1000000)
            else:
                try:
                    return int(float(count))
                except:
                    pass
    except Exception as e:
        print(f"âŒ")
    return None

def try_openalex(username: str):
    """ä½¿ç”¨ OpenAlex å­¦æœ¯æ•°æ®åº“ (å¯èƒ½æ”¶å½•ç ”ç©¶è€…)"""
    print("    å°è¯• OpenAlex...", end=" ")
    try:
        url = f"https://api.openalex.org/authors?search={username}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)

        context = ssl.create_default_context()
        with urllib.request.urlopen(req, timeout=10, context=context) as r:
            data = json.loads(r.read().decode('utf-8'))

        # æŸ¥æ‰¾ Twitter ä¿¡æ¯
        for result in data.get('results', []):
            twitter = result.get('ids', {}).get('twitter')
            if twitter and username.lower() in twitter.lower():
                # å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›ä¼°ç®—å€¼
                works = result.get('works_count', 0)
                return None  # OpenAlex ä¸æä¾›ç²‰ä¸æ•°
    except Exception as e:
        print(f"âŒ")
    return None

def try_wikipedia(username: str, name: str):
    """ä» Wikipedia ä¿¡æ¯æ¡†è·å–"""
    print("    å°è¯• Wikipedia...", end=" ")
    try:
        # å°è¯•æŸ¥æ‰¾ Wikipedia é¡µé¢
        search_terms = [name, username]

        for term in search_terms:
            url = f"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={urllib.parse.quote(term)}&format=json"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Research Project)'
            }
            req = urllib.request.Request(url, headers=headers)

            context = ssl.create_default_context()
            with urllib.request.urlopen(req, timeout=10, context=context) as r:
                data = json.loads(r.read().decode('utf-8'))

            # æŸ¥æ‰¾ç›¸å…³é¡µé¢
            for result in data.get('query', {}).get('search', []):
                page_title = result.get('title', '')

                # è·å–é¡µé¢å†…å®¹
                page_url = f"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&titles={urllib.parse.quote(page_title)}&rvprop=content&format=json"
                req2 = urllib.request.Request(page_url, headers=headers)

                with urllib.request.urlopen(req2, timeout=10, context=context) as r2:
                    page_data = json.loads(r2.read().decode('utf-8'))

                # åœ¨é¡µé¢å†…å®¹ä¸­æŸ¥æ‰¾ Twitter ç²‰ä¸æ•°
                pages = page_data.get('query', {}).get('pages', {})
                for page_id, page_info in pages.items():
                    revisions = page_info.get('revisions', [])
                    if revisions:
                        content = revisions[0].get('*', '')
                        # æŸ¥æ‰¾ twitter followers æ¨¡å¼
                        match = re.search(r'twitter.*?followers?.*?=\s*([\d,]+)', content, re.IGNORECASE)
                        if match:
                            return int(match.group(1).replace(',', ''))
    except Exception as e:
        print(f"âŒ")
    return None

def crawl_x_free(username: str, name: str = None):
    """å°è¯•æ‰€æœ‰æœ€åçš„å…è´¹æ–¹æ³•"""
    print(f"\nğŸ¦ æœ€åå°è¯•: @{username}")
    print("-" * 50)

    results = []

    # å°è¯•å„ç§æ–¹æ³•
    methods = [
        ("DuckDuckGo", try_duckduckgo),
        ("Bing", try_bing_search),
        ("Yandex", try_yandex_search),
    ]

    if name:
        methods.append(("Wikipedia", lambda u: try_wikipedia(u, name)))

    for method_name, method_func in methods:
        result = method_func(username)
        if result:
            print(f"âœ… {result:,} followers")
            results.append({
                'method': method_name,
                'followers': result
            })
        else:
            print(f"âŒ")

    return results

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ X/Twitter æœ€åå°è¯• - åˆ›æ„æ–¹æ³•")
    print("="*60)

    accounts = [
        ("MKBHD", "Marques Brownlee"),
        ("MrBeast", "MrBeast"),
        ("joerogan", "Joe Rogan"),
    ]

    for username, name in accounts:
        results = crawl_x_free(username, name)

        if results:
            print(f"\nâœ… @{username} æˆåŠŸè·å–æ•°æ®:")
            for r in results:
                print(f"   {r['method']}: {r['followers']:,}")
        else:
            print(f"\nâŒ @{username} æ‰€æœ‰æ–¹æ³•å‡å¤±è´¥")

        print()

    print("="*60)
    print("âš ï¸  ç»“è®º: Twitter/X çš„åçˆ¬è¿‡äºå¼ºå¤§")
    print("   å…è´¹æ–¹æ³•å·²æ— æ³•è·å–æ•°æ®")
    print("="*60)
