#!/usr/bin/env python3
"""
X/Twitter å…è´¹çˆ¬è™« - ç»ˆæå°è¯•
å°è¯•æ‰€æœ‰å¯èƒ½çš„å…è´¹æ–¹æ³•ï¼Œä¸ä¾èµ–API
"""

import os
import sys
import json
import re
import ssl
import time
import urllib.request
import urllib.parse
from datetime import datetime
from typing import Dict, List, Optional

class XFreeCrawler:
    """X/Twitter å…è´¹çˆ¬è™«"""

    def __init__(self):
        self.ssl_context = ssl.create_default_context()
        self.results = []

    def _get_headers(self, mobile=False) -> Dict:
        """è·å–è¯·æ±‚å¤´"""
        if mobile:
            return {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
            }
        else:
            return {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }

    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—"""
        if not text:
            return 0
        text = text.lower().replace(',', '').strip()
        multipliers = {'k': 1000, 'm': 1000000, 'b': 1000000000}
        for suffix, multiplier in multipliers.items():
            if suffix in text:
                try:
                    return int(float(text.replace(suffix, '').strip()) * multiplier)
                except:
                    return 0
        numbers = re.findall(r'[\d.]+', text)
        if numbers:
            try:
                return int(float(numbers[0]))
            except:
                pass
        return 0

    # ============ æ–¹æ³•1: æœ€æ–°Nitteré•œåƒ ============
    def try_nitter_mirrors(self, username: str) -> Dict:
        """å°è¯•æœ€æ–°çš„Nitteré•œåƒ"""
        print(f"    å°è¯• Nitter é•œåƒ...")

        # 2024å¹´æœ€æ–°Nitteré•œåƒåˆ—è¡¨
        nitter_instances = [
            "https://nitter.net",
            "https://nitter.privacydev.net",
            "https://nitter.freedit.eu",
            "https://nitter.poast.org",
            "https://nitter.datura.network",
            "https://nitter.projectsegfault.com",
            "https://nitter.perennialte.ch",
            "https://nitter.moomoo.me",
            "https://nitter.42l.fr",
            "https://nitter.nixnet.services",
            "https://nitter.pussthecat.org",
            "https://nitter.nohost.network",
            "https://nitter.tux.pizza",
            "https://nitter.foss.frederic.moe",
            "https://nitter.eu",
            "https://nitter.cz",
            "https://nitter.it",
            "https://nitter.es",
            "https://nitter.se",
            "https://nitter.nl",
        ]

        for instance in nitter_instances:
            try:
                url = f"{instance}/{username}"
                req = urllib.request.Request(url, headers=self._get_headers())

                with urllib.request.urlopen(req, timeout=10, context=self.ssl_context) as response:
                    html = response.read().decode('utf-8', errors='ignore')

                # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
                if any(x in html.lower() for x in ['rate limit', 'captcha', 'blocked', 'cloudflare']):
                    continue

                # æå–ç²‰ä¸æ•°
                followers_match = re.search(r'([\d,.]+[KMBk]?)\s*followers?', html, re.IGNORECASE)
                followers = self._parse_number(followers_match.group(1)) if followers_match else 0

                # æå–æ¨æ–‡æ•°
                tweets_match = re.search(r'([\d,.]+[KMBk]?)\s*tweets?', html, re.IGNORECASE)
                tweets = self._parse_number(tweets_match.group(1)) if tweets_match else 0

                # æå–æœ€è¿‘æ¨æ–‡
                recent_tweets = self._extract_tweets_from_nitter(html)

                if followers > 0:
                    return {
                        'status': 'success',
                        'method': 'nitter',
                        'source': instance,
                        'followers': followers,
                        'tweets_count': tweets,
                        'recent_tweets': recent_tweets,
                        'url': url
                    }

            except Exception as e:
                continue

        return {'status': 'failed', 'method': 'nitter', 'error': 'All mirrors blocked'}

    def _extract_tweets_from_nitter(self, html: str) -> List[Dict]:
        """ä»Nitter HTMLæå–æ¨æ–‡"""
        tweets = []
        try:
            # Nitteræ¨æ–‡é€šå¸¸åœ¨.timeline-itemä¸­
            tweet_pattern = r'<div class="timeline-item"[^>]*>.*?<div class="tweet-content"[^>]*>(.*?)</div>.*?</div>'
            matches = re.findall(tweet_pattern, html, re.DOTALL)

            for match in matches[:10]:
                # æå–æ–‡æœ¬
                text_match = re.search(r'<div class="tweet-content media-body"[^>]*>(.*?)</div>', match, re.DOTCASE)
                if text_match:
                    text = re.sub(r'<[^>]+>', '', text_match.group(1))
                    tweets.append({
                        'text': text[:200],
                        'date': 'unknown'
                    })
        except:
            pass
        return tweets

    # ============ æ–¹æ³•2: RSSæ¡¥æ¥æœåŠ¡ ============
    def try_rss_bridges(self, username: str) -> Dict:
        """å°è¯•RSSæ¡¥æ¥æœåŠ¡"""
        print(f"    å°è¯• RSS æ¡¥æ¥...")

        rss_services = [
            f"https://r.jina.ai/http://twitter.com/{username}",
            f"https://r.jina.ai/http://nitter.net/{username}",
            f"https://r.jina.ai/http://x.com/{username}",
        ]

        for url in rss_services:
            try:
                req = urllib.request.Request(url, headers=self._get_headers())

                with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as response:
                    content = response.read().decode('utf-8', errors='ignore')

                # å°è¯•æå–ç²‰ä¸æ•°
                follower_patterns = [
                    r'([\d,.]+[KMBk]?)\s*followers?',
                    r'Followers?\s*:?\s*([\d,.]+[KMBk]?)',
                    r'([\d,]+)\s*followers',
                ]

                for pattern in follower_patterns:
                    match = re.search(pattern, content, re.IGNORECASE)
                    if match:
                        followers = self._parse_number(match.group(1))
                        if followers > 1000:  # ç¡®ä¿æ˜¯åˆç†çš„æ•°å­—
                            return {
                                'status': 'success',
                                'method': 'rss_bridge',
                                'source': url.split('/')[2],
                                'followers': followers,
                                'content_preview': content[:500],
                                'url': f"https://twitter.com/{username}"
                            }

            except Exception as e:
                continue

        return {'status': 'failed', 'method': 'rss_bridge', 'error': 'No data found'}

    # ============ æ–¹æ³•3: ç¬¬ä¸‰æ–¹èšåˆæœåŠ¡ ============
    def try_third_party_services(self, username: str) -> Dict:
        """å°è¯•ç¬¬ä¸‰æ–¹èšåˆæœåŠ¡"""
        print(f"    å°è¯•ç¬¬ä¸‰æ–¹æœåŠ¡...")

        services = [
            # Social Blade
            f"https://socialblade.com/twitter/user/{username}",
            # SimilarWeb
            f"https://www.similarweb.com/website/twitter.com/#{username}",
        ]

        for url in services:
            try:
                req = urllib.request.Request(url, headers=self._get_headers())

                with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as response:
                    html = response.read().decode('utf-8', errors='ignore')

                # Social Blade æ¨¡å¼
                if 'socialblade' in url:
                    match = re.search(r'([\d,]+)\s*Followers', html)
                    if match:
                        followers = int(match.group(1).replace(',', ''))
                        return {
                            'status': 'success',
                            'method': 'socialblade',
                            'followers': followers,
                            'url': url
                        }

            except Exception as e:
                continue

        return {'status': 'failed', 'method': 'third_party', 'error': 'Services unavailable'}

    # ============ æ–¹æ³•4: ç›´æ¥ç½‘é¡µæŠ“å– (ç§»åŠ¨ç«¯) ============
    def try_mobile_web(self, username: str) -> Dict:
        """å°è¯•ç§»åŠ¨ç«¯ç½‘é¡µ"""
        print(f"    å°è¯•ç§»åŠ¨ç«¯ç½‘é¡µ...")

        urls = [
            f"https://mobile.twitter.com/{username}",
            f"https://m.twitter.com/{username}",
            f"https://twitter.com/i/user/{username}",
        ]

        for url in urls:
            try:
                req = urllib.request.Request(url, headers=self._get_headers(mobile=True))

                with urllib.request.urlopen(req, timeout=10, context=self.ssl_context) as response:
                    html = response.read().decode('utf-8', errors='ignore')

                # å°è¯•æå–ç²‰ä¸æ•°
                patterns = [
                    r'([\d,.]+[KMBk]?)\s*[Ff]ollowers?',
                    r'"followers_count":(\d+)',
                    r'"user_followers":(\d+)',
                ]

                for pattern in patterns:
                    match = re.search(pattern, html)
                    if match:
                        followers = self._parse_number(match.group(1))
                        if followers > 1000:
                            return {
                                'status': 'success',
                                'method': 'mobile_web',
                                'followers': followers,
                                'url': url
                            }

            except Exception as e:
                continue

        return {'status': 'failed', 'method': 'mobile_web', 'error': 'Blocked or changed'}

    # ============ æ–¹æ³•5: ç¼“å­˜æœåŠ¡ ============
    def try_cache_services(self, username: str) -> Dict:
        """å°è¯•ç¼“å­˜æœåŠ¡"""
        print(f"    å°è¯•ç¼“å­˜æœåŠ¡...")

        cache_urls = [
            f"https://webcache.googleusercontent.com/search?q=twitter.com/{username}",
            f"https://web.archive.org/web/2024*/https://twitter.com/{username}",
        ]

        for url in cache_urls:
            try:
                req = urllib.request.Request(url, headers=self._get_headers())

                with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as response:
                    html = response.read().decode('utf-8', errors='ignore')

                # æå–ç²‰ä¸æ•°
                match = re.search(r'([\d,.]+[KMBk]?)\s*[Ff]ollowers?', html)
                if match:
                    followers = self._parse_number(match.group(1))
                    if followers > 1000:
                        return {
                            'status': 'success',
                            'method': 'cache',
                            'source': 'google_cache' if 'google' in url else 'wayback',
                            'followers': followers,
                            'note': 'Data may be outdated',
                            'url': url
                        }

            except Exception as e:
                continue

        return {'status': 'failed', 'method': 'cache', 'error': 'No cached data'}

    # ============ ä¸»æŠ“å–å‡½æ•° ============
    def fetch(self, username: str) -> Dict:
        """å°è¯•æ‰€æœ‰å…è´¹æ–¹æ³•"""
        print(f"\n{'='*60}")
        print(f"ğŸ¦ æŠ“å– X/Twitter: @{username}")
        print('='*60)

        methods = [
            ("Nitteré•œåƒ", self.try_nitter_mirrors),
            ("RSSæ¡¥æ¥", self.try_rss_bridges),
            ("ç¬¬ä¸‰æ–¹æœåŠ¡", self.try_third_party_services),
            ("ç§»åŠ¨ç«¯ç½‘é¡µ", self.try_mobile_web),
            ("ç¼“å­˜æœåŠ¡", self.try_cache_services),
        ]

        for method_name, method_func in methods:
            print(f"\n  æ–¹æ³•: {method_name}")
            result = method_func(username)

            if result.get('status') == 'success':
                print(f"  âœ… æˆåŠŸ!")
                print(f"     ç²‰ä¸: {result.get('followers', 0):,}")
                print(f"     æ–¹æ³•: {result.get('method', 'unknown')}")
                if result.get('source'):
                    print(f"     æ¥æº: {result.get('source')}")
                return result
            else:
                print(f"  âŒ {result.get('error', 'Failed')}")

        # å…¨éƒ¨å¤±è´¥
        print(f"\n  âš ï¸  æ‰€æœ‰å…è´¹æ–¹æ³•éƒ½å¤±è´¥äº†")
        return {
            'status': 'failed',
            'followers': 0,
            'error': 'All free methods failed. X/Twitter anti-scraping is too strong.'
        }


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ X/Twitter å…è´¹çˆ¬è™« - ç»ˆæå°è¯•")
    print("å°è¯•æ‰€æœ‰å¯èƒ½çš„å…è´¹æ–¹æ³•")
    print("="*70)

    usernames = ['MKBHD', 'MrBeast', 'joerogan']
    crawler = XFreeCrawler()
    results = []

    for username in usernames:
        result = crawler.fetch(username)
        results.append({
            'username': username,
            'result': result
        })
        time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿ

    # ä¿å­˜ç»“æœ
    output_dir = ".."
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{output_dir}/data/json/X_CRAWLER_RESULTS_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    # æ‘˜è¦
    print("\n" + "="*70)
    print("ğŸ“Š ç»“æœæ‘˜è¦")
    print("="*70)

    for r in results:
        status = "âœ…" if r['result']['status'] == 'success' else "âŒ"
        followers = r['result'].get('followers', 0)
        method = r['result'].get('method', 'failed')
        print(f"{status} @{r['username']:<15} | {followers:>12,} | {method}")

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")
    print("="*70)


if __name__ == "__main__":
    main()
